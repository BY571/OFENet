{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "english-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-junction",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "renewable-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, torch.tanh(mu)\n",
    "        \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            hidden_size (int): Number of nodes in the network layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state_action))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "powerful-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OFENet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, target_dim, num_layer=4, hidden_size=40):\n",
    "        super(OFENet, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.target_dim = target_dim\n",
    "        \n",
    "        denseblock = DenseNetBlock\n",
    "        state_layer = []\n",
    "        action_layer = []\n",
    "        \n",
    "        for i in range(num_layer):\n",
    "            state_layer += [denseblock(input_nodes=state_size+i*hidden_size,\n",
    "                                       output_nodes=hidden_size,\n",
    "                                       activation=\"SiLU\",\n",
    "                                       batch_norm=True)]\n",
    "            \n",
    "        self.state_layer_block = nn.Sequential(*state_layer)\n",
    "        self.encode_state_out = state_size + (num_layer) * hidden_size\n",
    "        action_block_input = self.encode_state_out + action_size\n",
    "        \n",
    "        for i in range(num_layer):\n",
    "            action_layer += [denseblock(input_nodes=action_block_input+i*hidden_size,\n",
    "                                       output_nodes=hidden_size,\n",
    "                                       activation=\"SiLU\",\n",
    "                                       batch_norm=True)]\n",
    "        self.action_layer_block = nn.Sequential(*action_layer)\n",
    "\n",
    "        self.pred_layer = nn.Linear((state_size+(2*num_layer)*hidden_size)+action_size, target_dim)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        features = state\n",
    "        features = self.state_layer_block(features)\n",
    "        features = torch.cat((features, action), dim=1)\n",
    "        features = self.action_layer_block(features)\n",
    "        pred = self.pred_layer(features)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def get_state_features(self, state):\n",
    "        self.state_layer_block.eval()\n",
    "        with torch.no_grad():\n",
    "            z0 = self.state_layer_block(state)\n",
    "        self.state_layer_block.train()\n",
    "        return z0\n",
    "    \n",
    "    def get_state_action_features(self, state, action):\n",
    "        self.state_layer_block.eval()\n",
    "        self.action_layer_block.eval()\n",
    "        with torch.no_grad():\n",
    "            z0 = self.state_layer_block(state)\n",
    "            action_cat = torch.cat((z0, action), dim=1)\n",
    "            z0_a = self.action_layer_block(action_cat)\n",
    "        self.state_layer_block.train()\n",
    "        self.action_layer_block.train()\n",
    "        return z0_a\n",
    "    \n",
    "    def train_ofenet(self, experiences, optim):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # ---------------------------- update OFENet ---------------------------- #\n",
    "        pred = self.forward(states, actions)\n",
    "        target_states = next_states[:, :self.target_dim]\n",
    "        ofenet_loss = (target_states - pred).pow(2).mean()\n",
    "        \n",
    "\n",
    "        optim.zero_grad()\n",
    "        ofenet_loss.backward()\n",
    "        optim.step()\n",
    "        return ofenet_loss.item()\n",
    "    \n",
    "    def get_action_state_dim(self,):\n",
    "        return (self.state_size+(2*self.num_layer)*self.hidden_size)+self.action_size\n",
    "    \n",
    "    def get_state_dim(self,):\n",
    "        return self.encode_state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "foreign-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetBlock(nn.Module):\n",
    "    def __init__(self, input_nodes, output_nodes, activation, batch_norm=False):\n",
    "        super(DenseNetBlock, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.do_batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Linear(input_nodes, output_nodes, bias=True)\n",
    "            nn.init.xavier_uniform_(self.layer.weight)\n",
    "            nn.init.zeros_(self.layer.bias)\n",
    "            self.batch_norm = nn.BatchNorm1d(output_nodes) #, momentum=0.99, eps=0.001\n",
    "        else:\n",
    "            self.layer = nn.Linear(input_nodes, output_nodes)\n",
    "        if activation == \"SiLU\":\n",
    "            self.act = nn.SiLU()\n",
    "        elif activation == \"ReLU\":\n",
    "            self.act = nn.ReLU()\n",
    "        else:\n",
    "            print(\"Activation Function can not be selected!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        features = self.layer(x)\n",
    "\n",
    "        if self.do_batch_norm:\n",
    "            features = self.batch_norm(features)\n",
    "        features = self.act(features)\n",
    "        assert features.shape[0] == identity_map.shape[0], \"features: {} | identity: {}\".format(features.shape, identity_map.shape)\n",
    "        features = torch.cat((features, identity_map), dim=1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "critical-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(steps, precollected, print_every=10):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    average_100_scores = []\n",
    "    scores = []\n",
    "    losses = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state.reshape((1, state_size))\n",
    "    score = 0\n",
    "    i_episode = 1\n",
    "    for step in range(precollected+1, steps+1):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        action_v = action.numpy()\n",
    "        action_v = np.clip(action_v, action_low, action_high)\n",
    "        next_state, reward, done, info = env.step(action_v)\n",
    "        next_state = next_state.reshape((1, state_size))\n",
    "        ofenet_loss, a_loss, c_loss = agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            scores_deque.append(score)\n",
    "            scores.append(score)\n",
    "            average_100_scores.append(np.mean(scores_deque))\n",
    "            losses.append((ofenet_loss, a_loss, c_loss))\n",
    "            print('\\rEpisode {} Frame: [{}/{}] Reward: {:.2f}  Average100 Score: {:.2f} ofenet_loss: {:.3f}, a_loss: {:.3f}, c_loss: {:.3f}'.format(i_episode, step, steps, score, np.mean(scores_deque), ofenet_loss, a_loss, c_loss))\n",
    "            state = env.reset()\n",
    "            state = state.reshape((1, state_size))\n",
    "            score = 0\n",
    "            i_episode += 1\n",
    "             \n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "important-bruce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=26, out_features=30, bias=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.ofenet.state_layer_block[0].layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "confused-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buffer(samples=1000):\n",
    "    collected_samples = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = state.reshape((1, state_size))\n",
    "    for i in range(samples):\n",
    "            \n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = next_state.reshape((1, state_size))\n",
    "        agent.memory.add(state, action, reward, next_state, done)\n",
    "        collected_samples += 1\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state.reshape((1, state_size))\n",
    "    print(\"Adding random samples to buffer done! Buffer size: \", agent.memory.__len__())\n",
    "                \n",
    "def pretrain_ofenet(agent, epochs):\n",
    "    losses = []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        states, actions, rewards, next_states, dones = agent.memory.sample()\n",
    "        # ---------------------------- update OFENet ---------------------------- #\n",
    "        pred = agent.ofenet.forward(states, actions)\n",
    "        targets = next_states[:,:17]\n",
    "        ofenet_loss = (targets-pred).pow(2).mean()\n",
    "        agent.ofenet_optim.zero_grad()\n",
    "        ofenet_loss.backward()\n",
    "        agent.ofenet_optim.step()\n",
    "        losses.append(ofenet_loss.item())\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    print(losses[-1])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-pepper",
   "metadata": {},
   "source": [
    "# Train SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-collectible",
   "metadata": {},
   "source": [
    "# Train OFENet REDQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "nonprofit-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REDQ_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, device, action_prior=\"uniform\", N=2, M=2, G=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.hidden_size = 256\n",
    "        \n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=lr) \n",
    "        self._action_prior = action_prior\n",
    "        self.alphas = []\n",
    "        print(\"Using: \", device)\n",
    "        \n",
    "        # REDQ parameter\n",
    "        self.N = N # number of critics in the ensemble\n",
    "        self.M = M # number of target critics that are randomly selected\n",
    "        self.G = G # Updates per step ~ UTD-ratio\n",
    "        \n",
    "        ofenet_size = 30\n",
    "        self.ofenet = OFENet(state_size, action_size, target_dim=17, num_layer=8, hidden_size=ofenet_size).to(device)\n",
    "        # TODO: CHECK ADAM PARAMS WITH TF AND PAPER\n",
    "        self.ofenet_optim = optim.Adam(self.ofenet.parameters(), lr=3e-4, eps=1e-07)  \n",
    "        print(self.ofenet)\n",
    "\n",
    "        # split state and action ~ weird step but to keep critic inputs consistent\n",
    "        feature_size = self.ofenet.get_state_dim()\n",
    "        feature_action_size = self.ofenet.get_action_state_dim()\n",
    "        \n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(feature_size, action_size, random_seed, hidden_size=self.hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr)     \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critics = []\n",
    "        self.target_critics = []\n",
    "        self.optims = []\n",
    "        for i in range(self.N):\n",
    "            critic = Critic(feature_action_size, i, hidden_size=self.hidden_size).to(device)\n",
    "\n",
    "            optimizer = optim.Adam(critic.parameters(), lr=lr, weight_decay=0)\n",
    "            self.optims.append(optimizer)\n",
    "            self.critics.append(critic)\n",
    "            target = Critic(feature_action_size, i, hidden_size=self.hidden_size).to(device)\n",
    "            self.target_critics.append(target)\n",
    "\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, random_seed)\n",
    "        \n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        actor_loss, critic1_loss, ofenet_loss = 0, 0, 0\n",
    "        for update in range(self.G):\n",
    "            if len(self.memory) > batch_size:\n",
    "                ofenet_loss = self.ofenet.train_ofenet(self.memory.sample(), self.ofenet_optim)\n",
    "                experiences = self.memory.sample()\n",
    "                actor_loss, critic1_loss = self.learn(update, experiences, gamma)\n",
    "        return ofenet_loss, actor_loss, critic1_loss # future ofenet_loss\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        self.ofenet.eval()\n",
    "        with torch.no_grad():\n",
    "            state = self.ofenet.get_state_features(state)\n",
    "            action, _, _ = self.actor_local.sample(state)\n",
    "        self.actor_local.train()\n",
    "        self.ofenet.train()\n",
    "        return action.detach().cpu()[0]\n",
    "    \n",
    "    def eval_(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        self.ofenet.eval()\n",
    "        with torch.no_grad():\n",
    "            state = self.ofenet.get_state_features(state)\n",
    "            _, _ , action = self.actor_local.sample(state)\n",
    "        self.actor_local.train()\n",
    "        self.ofenet.train()\n",
    "        return action.detach().cpu()[0]\n",
    "    \n",
    "    def learn(self, step, experiences, gamma):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * (min_critic_target(next_state, actor_target(next_state)) - α *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss = α * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # sample target critics\n",
    "        idx = np.random.choice(len(self.critics), self.M, replace=False) # replace=False so that not picking the same idx twice\n",
    "        \n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            next_state_features = self.ofenet.get_state_features(next_states)\n",
    "            next_action, next_log_prob, _ = self.actor_local.sample(next_state_features)\n",
    "            next_state_action_features = self.ofenet.get_state_action_features(next_states, next_action) #get_state_action_features\n",
    "            # TODO: make this variable for possible more than tnext_state_action_featureswo target critics\n",
    "            Q_target1_next = self.target_critics[idx[0]](next_state_action_features)\n",
    "            Q_target2_next = self.target_critics[idx[1]](next_state_action_features)\n",
    "            \n",
    "            # take the min of both critics for updating\n",
    "            Q_target_next = torch.min(Q_target1_next, Q_target2_next) - self.alpha.to(device) * next_log_prob\n",
    "\n",
    "        Q_targets = 5.0*rewards.cpu() + (gamma * (1 - dones.cpu()) * Q_target_next.cpu())\n",
    "\n",
    "        # Compute critic losses and update critics \n",
    "        state_action_features = self.ofenet.get_state_action_features(states, actions)\n",
    "        for critic, optim, target in zip(self.critics, self.optims, self.target_critics):\n",
    "            Q = critic(state_action_features).cpu()\n",
    "            Q_loss = 0.5*F.mse_loss(Q, Q_targets)\n",
    "        \n",
    "            # Update critic\n",
    "            optim.zero_grad()\n",
    "            Q_loss.backward()\n",
    "            optim.step()\n",
    "            # soft update of the targets\n",
    "            self.soft_update(critic, target)\n",
    "        \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        if step == self.G-1:\n",
    "            state_features = self.ofenet.get_state_features(states)\n",
    "            actions_pred, log_prob, _ = self.actor_local.sample(state_features)             \n",
    "            \n",
    "            state_action_features = self.ofenet.get_state_action_features(states, actions_pred)\n",
    "            # TODO: make this variable for possible more than two critics\n",
    "            Q1 = self.critics[idx[0]](state_action_features).cpu()\n",
    "            Q2 = self.critics[idx[0]](state_action_features).cpu()\n",
    "            Q = torch.min(Q1,Q2)\n",
    "            actor_loss = (self.alpha * log_prob.cpu() - Q).mean()\n",
    "            # Optimize the actor loss\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Compute alpha loss \n",
    "            alpha_loss = - (self.log_alpha.exp() * (log_prob.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp().detach()\n",
    "            self.alphas.append(self.alpha.detach())\n",
    "            \n",
    "        return actor_loss.item(), Q_loss.item()\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "derived-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cpu\n",
      "OFENet(\n",
      "  (state_layer_block): Sequential(\n",
      "    (0): DenseNetBlock(\n",
      "      (layer): Linear(in_features=26, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (1): DenseNetBlock(\n",
      "      (layer): Linear(in_features=56, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (2): DenseNetBlock(\n",
      "      (layer): Linear(in_features=86, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (3): DenseNetBlock(\n",
      "      (layer): Linear(in_features=116, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (4): DenseNetBlock(\n",
      "      (layer): Linear(in_features=146, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (5): DenseNetBlock(\n",
      "      (layer): Linear(in_features=176, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (6): DenseNetBlock(\n",
      "      (layer): Linear(in_features=206, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (7): DenseNetBlock(\n",
      "      (layer): Linear(in_features=236, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (action_layer_block): Sequential(\n",
      "    (0): DenseNetBlock(\n",
      "      (layer): Linear(in_features=272, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (1): DenseNetBlock(\n",
      "      (layer): Linear(in_features=302, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (2): DenseNetBlock(\n",
      "      (layer): Linear(in_features=332, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (3): DenseNetBlock(\n",
      "      (layer): Linear(in_features=362, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (4): DenseNetBlock(\n",
      "      (layer): Linear(in_features=392, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (5): DenseNetBlock(\n",
      "      (layer): Linear(in_features=422, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (6): DenseNetBlock(\n",
      "      (layer): Linear(in_features=452, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (7): DenseNetBlock(\n",
      "      (layer): Linear(in_features=482, out_features=30, bias=True)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (pred_layer): Linear(in_features=512, out_features=17, bias=True)\n",
      ")\n",
      "Trainable OFENet Parameter:  132081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\" #\"HalfCheetahPyBulletEnv-v0\"#\"Pendulum-v0\"\n",
    "max_steps = 1_000_000\n",
    "seed = 1\n",
    "#Hyperparameter\n",
    "lr = 3e-4\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 256\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "\n",
    "random_collect = 10000\n",
    "\n",
    "# RED-Q Parameter\n",
    "N = 2\n",
    "M = 2\n",
    "G = 1\n",
    "\n",
    "#writer = SummaryWriter(\"runs/\"+args.info)\n",
    "env = gym.make(env_name)\n",
    "action_high = env.action_space.high[0]\n",
    "action_low = env.action_space.low[0]\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "agent = REDQ_Agent(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              random_seed=seed,\n",
    "              device=device,\n",
    "              action_prior=\"uniform\", N=N, M=M, G=G)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable OFENet Parameter: \", count_parameters(agent.ofenet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "patent-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding random samples to buffer done! Buffer size:  10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00331158097833395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "fill_buffer(samples=random_collect)\n",
    "start_time = time.time()\n",
    "agent = pretrain_ofenet(agent, epochs=random_collect)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "racial-metabolism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training took: 3.44853032430013\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-training took: {}\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-wilson",
   "metadata": {},
   "source": [
    "# paper achieves loss of ~ 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "finite-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.ofenet.state_dict(), \"ofenet_params_cheetah.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "electoral-bullet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OFENet(\n",
       "  (state_layer_block): Sequential(\n",
       "    (0): DenseNetBlock(\n",
       "      (layer): Linear(in_features=26, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (1): DenseNetBlock(\n",
       "      (layer): Linear(in_features=56, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (2): DenseNetBlock(\n",
       "      (layer): Linear(in_features=86, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (3): DenseNetBlock(\n",
       "      (layer): Linear(in_features=116, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (4): DenseNetBlock(\n",
       "      (layer): Linear(in_features=146, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (5): DenseNetBlock(\n",
       "      (layer): Linear(in_features=176, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (6): DenseNetBlock(\n",
       "      (layer): Linear(in_features=206, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (7): DenseNetBlock(\n",
       "      (layer): Linear(in_features=236, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (action_layer_block): Sequential(\n",
       "    (0): DenseNetBlock(\n",
       "      (layer): Linear(in_features=272, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (1): DenseNetBlock(\n",
       "      (layer): Linear(in_features=302, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (2): DenseNetBlock(\n",
       "      (layer): Linear(in_features=332, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (3): DenseNetBlock(\n",
       "      (layer): Linear(in_features=362, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (4): DenseNetBlock(\n",
       "      (layer): Linear(in_features=392, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (5): DenseNetBlock(\n",
       "      (layer): Linear(in_features=422, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (6): DenseNetBlock(\n",
       "      (layer): Linear(in_features=452, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (7): DenseNetBlock(\n",
       "      (layer): Linear(in_features=482, out_features=30, bias=True)\n",
       "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (pred_layer): Linear(in_features=512, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.ofenet.load_state_dict(torch.load(\"ofenet_params_cheetah.pth\"))\n",
    "agent.ofenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "manufactured-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Frame: [11000/1000000] Reward: -1095.94  Average100 Score: -1095.94 ofenet_loss: 0.003, a_loss: 12.023, c_loss: 9.186\n",
      "Episode 2 Frame: [12000/1000000] Reward: -1154.81  Average100 Score: -1125.38 ofenet_loss: 0.003, a_loss: 30.677, c_loss: 14.340\n",
      "Episode 3 Frame: [13000/1000000] Reward: -1317.20  Average100 Score: -1189.32 ofenet_loss: 0.003, a_loss: 52.550, c_loss: 18.166\n",
      "Episode 4 Frame: [14000/1000000] Reward: -1231.73  Average100 Score: -1199.92 ofenet_loss: 0.004, a_loss: 79.028, c_loss: 19.346\n",
      "Episode 5 Frame: [15000/1000000] Reward: -1371.70  Average100 Score: -1234.28 ofenet_loss: 0.003, a_loss: 107.285, c_loss: 26.071\n",
      "Episode 6 Frame: [16000/1000000] Reward: -1027.28  Average100 Score: -1199.78 ofenet_loss: 0.004, a_loss: 130.426, c_loss: 19.074\n",
      "Episode 7 Frame: [17000/1000000] Reward: -1238.18  Average100 Score: -1205.26 ofenet_loss: 0.004, a_loss: 156.055, c_loss: 23.853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-dfaac6d5b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_collect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training took {} min!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-94c5c379b4c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(steps, precollected, print_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mofenet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-6136ec32e83f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mofenet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ofenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mofenet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic1_loss\u001b[0m \u001b[0;31m# future ofenet_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-6136ec32e83f>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, step, experiences, gamma)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# Update critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mQ_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# soft update of the targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "scores = train(max_steps, random_collect)\n",
    "t1 = time.time()\n",
    "env.close()\n",
    "print(\"training took {} min!\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-assets",
   "metadata": {},
   "source": [
    "# Actor loss is the problem!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
