{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "alternative-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-arthur",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rolled-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, torch.tanh(mu)\n",
    "        \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            hidden_size (int): Number of nodes in the network layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state_action))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "inappropriate-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OFENet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, num_layer=4, hidden_size=40):\n",
    "        super(OFENet, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        denseblock = DenseNetBlock\n",
    "        state_layer = []\n",
    "        action_layer = []\n",
    "        \n",
    "        for i in range(num_layer):\n",
    "            state_layer += [denseblock(input_nodes=state_size+i*hidden_size,\n",
    "                                       output_nodes=hidden_size,\n",
    "                                       activation=\"SiLU\",\n",
    "                                       batch_norm=True)]\n",
    "            \n",
    "        self.state_layer_block = nn.Sequential(*state_layer)\n",
    "        self.encode_state_out = state_size + (num_layer) * hidden_size\n",
    "        action_block_input = self.encode_state_out + action_size\n",
    "        \n",
    "        for i in range(num_layer):\n",
    "            action_layer += [denseblock(input_nodes=action_block_input+i*hidden_size,\n",
    "                                       output_nodes=hidden_size,\n",
    "                                       activation=\"SiLU\",\n",
    "                                       batch_norm=True)]\n",
    "        self.action_layer_block = nn.Sequential(*action_layer)\n",
    "\n",
    "        self.pred_layer = nn.Linear((state_size+(2*num_layer)*hidden_size)+action_size, state_size)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        features = state\n",
    "        features = self.state_layer_block(features)\n",
    "        features = torch.cat((features, action), dim=1)\n",
    "        features = self.action_layer_block(features)\n",
    "        pred = self.pred_layer(features)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def get_state_features(self, state):\n",
    "        self.state_layer_block.eval()\n",
    "        with torch.no_grad():\n",
    "            z0 = self.state_layer_block(state)\n",
    "        self.state_layer_block.train()\n",
    "        return z0\n",
    "    \n",
    "    def get_state_action_features(self, state, action):\n",
    "        self.state_layer_block.eval()\n",
    "        self.action_layer_block.eval()\n",
    "        with torch.no_grad():\n",
    "            z0 = self.state_layer_block(state)\n",
    "            action_cat = torch.cat((z0, action), dim=1)\n",
    "            z0_a = self.action_layer_block(action_cat)\n",
    "        self.state_layer_block.train()\n",
    "        self.action_layer_block.train()\n",
    "        return z0_a\n",
    "    \n",
    "    def train_ofenet(self, experiences, optim):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # ---------------------------- update OFENet ---------------------------- #\n",
    "        pred = self.forward(states, actions)\n",
    "        target_states = next_states#[:, :self.state_size]\n",
    "        ofenet_loss = (target_states - pred).pow(2).mean()\n",
    "        \n",
    "\n",
    "        optim.zero_grad()\n",
    "        ofenet_loss.backward()\n",
    "        optim.step()\n",
    "        return ofenet_loss.item()\n",
    "    \n",
    "    def get_action_state_dim(self,):\n",
    "        return (self.state_size+(2*self.num_layer)*self.hidden_size)+self.action_size\n",
    "    \n",
    "    def get_state_dim(self,):\n",
    "        return self.encode_state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "removed-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetBlock(nn.Module):\n",
    "    def __init__(self, input_nodes, output_nodes, activation, batch_norm=False):\n",
    "        super(DenseNetBlock, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.do_batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Linear(input_nodes, output_nodes, bias=False)\n",
    "            nn.init.xavier_uniform_(self.layer.weight)\n",
    "            #nn.init.zeros_(self.layer.bias)\n",
    "            self.batch_norm = nn.BatchNorm1d(output_nodes) #, momentum=0.99, eps=0.001\n",
    "        else:\n",
    "            self.layer = nn.Linear(input_nodes, output_nodes)\n",
    "        if activation == \"SiLU\":\n",
    "            self.act = nn.SiLU()\n",
    "        elif activation == \"ReLU\":\n",
    "            self.act = nn.ReLU()\n",
    "        else:\n",
    "            print(\"Activation Function can not be selected!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        features = self.layer(x)\n",
    "\n",
    "        if self.do_batch_norm:\n",
    "            features = self.batch_norm(features)\n",
    "        features = self.act(features)\n",
    "        assert features.shape[0] == identity_map.shape[0], \"features: {} | identity: {}\".format(features.shape, identity_map.shape)\n",
    "        features = torch.cat((features, identity_map), dim=1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "christian-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(steps, precollected, print_every=10):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    average_100_scores = []\n",
    "    scores = []\n",
    "    losses = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state.reshape((1, state_size))\n",
    "    score = 0\n",
    "    i_episode = 1\n",
    "    for step in range(precollected+1, steps+1):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        action_v = action.numpy()\n",
    "        action_v = np.clip(action_v, action_low, action_high)\n",
    "        next_state, reward, done, info = env.step(action_v)\n",
    "        next_state = next_state.reshape((1, state_size))\n",
    "        ofenet_loss, a_loss, c_loss = agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            scores_deque.append(score)\n",
    "            scores.append(score)\n",
    "            average_100_scores.append(np.mean(scores_deque))\n",
    "            losses.append((ofenet_loss, a_loss, c_loss))\n",
    "            print('\\rEpisode {} Frame: [{}/{}] Reward: {:.2f}  Average100 Score: {:.2f} ofenet_loss: {:.3f}, a_loss: {:.3f}, c_loss: {:.3f}'.format(i_episode, step, steps, score, np.mean(scores_deque), ofenet_loss, a_loss, c_loss))\n",
    "            state = env.reset()\n",
    "            state = state.reshape((1, state_size))\n",
    "            score = 0\n",
    "            i_episode += 1\n",
    "             \n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "known-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buffer(samples=1000):\n",
    "    collected_samples = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = state.reshape((1, state_size))\n",
    "    for i in range(samples):\n",
    "            \n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = next_state.reshape((1, state_size))\n",
    "        agent.memory.add(state, action, reward, next_state, done)\n",
    "        collected_samples += 1\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state.reshape((1, state_size))\n",
    "    print(\"Adding random samples to buffer done! Buffer size: \", agent.memory.__len__())\n",
    "                \n",
    "def pretrain_ofenet(agent, epochs):\n",
    "    losses = []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        states, actions, rewards, next_states, dones = agent.memory.sample()\n",
    "        # ---------------------------- update OFENet ---------------------------- #\n",
    "        pred = agent.ofenet.forward(states, actions)\n",
    "        ofenet_loss = (pred - next_states).pow(2).mean()\n",
    "        agent.ofenet_optim.zero_grad()\n",
    "        ofenet_loss.backward()\n",
    "        agent.ofenet_optim.step()\n",
    "        losses.append(ofenet_loss.item())\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    print(losses[-1])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-kruger",
   "metadata": {},
   "source": [
    "# Train SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-manhattan",
   "metadata": {},
   "source": [
    "# Train OFENet REDQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "initial-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REDQ_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, device, action_prior=\"uniform\", N=2, M=2, G=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.hidden_size = 256\n",
    "        \n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=lr) \n",
    "        self._action_prior = action_prior\n",
    "        self.alphas = []\n",
    "        print(\"Using: \", device)\n",
    "        \n",
    "        # REDQ parameter\n",
    "        self.N = N # number of critics in the ensemble\n",
    "        self.M = M # number of target critics that are randomly selected\n",
    "        self.G = G # Updates per step ~ UTD-ratio\n",
    "        \n",
    "        ofenet_size = 30\n",
    "        self.ofenet = OFENet(state_size, action_size, num_layer=8, hidden_size=ofenet_size).to(device)\n",
    "        # TODO: CHECK ADAM PARAMS WITH TF AND PAPER\n",
    "        self.ofenet_optim = optim.Adam(self.ofenet.parameters(), lr=3e-4)  \n",
    "        print(self.ofenet)\n",
    "\n",
    "        # split state and action ~ weird step but to keep critic inputs consistent\n",
    "        feature_size = self.ofenet.get_state_dim()\n",
    "        feature_action_size = self.ofenet.get_action_state_dim()\n",
    "        \n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(feature_size, action_size, random_seed, hidden_size=self.hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr)     \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critics = []\n",
    "        self.target_critics = []\n",
    "        self.optims = []\n",
    "        for i in range(self.N):\n",
    "            critic = Critic(feature_action_size, i, hidden_size=self.hidden_size).to(device)\n",
    "\n",
    "            optimizer = optim.Adam(critic.parameters(), lr=lr, weight_decay=0)\n",
    "            self.optims.append(optimizer)\n",
    "            self.critics.append(critic)\n",
    "            target = Critic(feature_action_size, i, hidden_size=self.hidden_size).to(device)\n",
    "            self.target_critics.append(target)\n",
    "\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, random_seed)\n",
    "        \n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        actor_loss, critic1_loss, ofenet_loss = 0, 0, 0\n",
    "        for update in range(self.G):\n",
    "            if len(self.memory) > batch_size:\n",
    "                ofenet_loss = self.ofenet.train_ofenet(self.memory.sample(), self.ofenet_optim)\n",
    "                experiences = self.memory.sample()\n",
    "                actor_loss, critic1_loss = self.learn(update, experiences, gamma)\n",
    "        return ofenet_loss, actor_loss, critic1_loss, # future ofenet_loss\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        self.ofenet.eval()\n",
    "        with torch.no_grad():\n",
    "            state = self.ofenet.get_state_features(state)\n",
    "            action, _, _ = self.actor_local.sample(state)\n",
    "        self.actor_local.train()\n",
    "        self.ofenet.train()\n",
    "        return action.detach().cpu()[0]\n",
    "    \n",
    "    def eval_(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        self.ofenet.eval()\n",
    "        with torch.no_grad():\n",
    "            state = self.ofenet.get_state_features(state)\n",
    "            _, _ , action = self.actor_local.sample(state)\n",
    "        self.actor_local.train()\n",
    "        self.ofenet.train()\n",
    "        return action.detach().cpu()[0]\n",
    "    \n",
    "    def learn(self, step, experiences, gamma):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * (min_critic_target(next_state, actor_target(next_state)) - α *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss = α * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # sample target critics\n",
    "        idx = np.random.choice(len(self.critics), self.M, replace=False) # replace=False so that not picking the same idx twice\n",
    "        \n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            next_state_features = self.ofenet.get_state_features(next_states)\n",
    "            next_action, next_log_prob, _ = self.actor_local.sample(next_state_features)\n",
    "            next_state_action_features = self.ofenet.get_state_action_features(next_states, next_action) #get_state_action_features\n",
    "            # TODO: make this variable for possible more than tnext_state_action_featureswo target critics\n",
    "            Q_target1_next = self.target_critics[idx[0]](next_state_action_features)\n",
    "            Q_target2_next = self.target_critics[idx[1]](next_state_action_features)\n",
    "            \n",
    "            # take the min of both critics for updating\n",
    "            Q_target_next = torch.min(Q_target1_next, Q_target2_next) - self.alpha.to(device) * next_log_prob\n",
    "\n",
    "        Q_targets = 5.0*rewards.cpu() + (gamma * (1 - dones.cpu()) * Q_target_next.cpu())\n",
    "\n",
    "        # Compute critic losses and update critics \n",
    "        state_action_features = self.ofenet.get_state_action_features(states, actions)\n",
    "        for critic, optim, target in zip(self.critics, self.optims, self.target_critics):\n",
    "            Q = critic(state_action_features).cpu()\n",
    "            Q_loss = 0.5*F.mse_loss(Q, Q_targets)\n",
    "        \n",
    "            # Update critic\n",
    "            optim.zero_grad()\n",
    "            Q_loss.backward()\n",
    "            optim.step()\n",
    "            # soft update of the targets\n",
    "            self.soft_update(critic, target)\n",
    "        \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        if step == self.G-1:\n",
    "            state_features = self.ofenet.get_state_features(states)\n",
    "            actions_pred, log_prob, _ = self.actor_local.sample(state_features)             \n",
    "            \n",
    "            state_action_features = self.ofenet.get_state_action_features(states, actions_pred)\n",
    "            # TODO: make this variable for possible more than two critics\n",
    "            Q1 = self.critics[idx[0]](state_action_features).cpu()\n",
    "            Q2 = self.critics[idx[0]](state_action_features).cpu()\n",
    "            Q = torch.min(Q1,Q2)\n",
    "\n",
    "            actor_loss = (self.alpha * log_prob.cpu() - Q).mean()\n",
    "            # Optimize the actor loss\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Compute alpha loss \n",
    "            alpha_loss = - (self.log_alpha.exp() * (log_prob.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp().detach()\n",
    "            self.alphas.append(self.alpha.detach())\n",
    "            \n",
    "        return actor_loss.item(), Q_loss.item()\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "indie-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n",
      "OFENet(\n",
      "  (state_layer_block): Sequential(\n",
      "    (0): DenseNetBlock(\n",
      "      (layer): Linear(in_features=26, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (1): DenseNetBlock(\n",
      "      (layer): Linear(in_features=56, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (2): DenseNetBlock(\n",
      "      (layer): Linear(in_features=86, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (3): DenseNetBlock(\n",
      "      (layer): Linear(in_features=116, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (4): DenseNetBlock(\n",
      "      (layer): Linear(in_features=146, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (5): DenseNetBlock(\n",
      "      (layer): Linear(in_features=176, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (6): DenseNetBlock(\n",
      "      (layer): Linear(in_features=206, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (7): DenseNetBlock(\n",
      "      (layer): Linear(in_features=236, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (action_layer_block): Sequential(\n",
      "    (0): DenseNetBlock(\n",
      "      (layer): Linear(in_features=272, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (1): DenseNetBlock(\n",
      "      (layer): Linear(in_features=302, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (2): DenseNetBlock(\n",
      "      (layer): Linear(in_features=332, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (3): DenseNetBlock(\n",
      "      (layer): Linear(in_features=362, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (4): DenseNetBlock(\n",
      "      (layer): Linear(in_features=392, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (5): DenseNetBlock(\n",
      "      (layer): Linear(in_features=422, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (6): DenseNetBlock(\n",
      "      (layer): Linear(in_features=452, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "    (7): DenseNetBlock(\n",
      "      (layer): Linear(in_features=482, out_features=30, bias=False)\n",
      "      (batch_norm): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (pred_layer): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n",
      "Trainable OFENet Parameter:  136218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\" #\"HalfCheetahPyBulletEnv-v0\"#\"Pendulum-v0\"\n",
    "max_steps = 1_000_000\n",
    "seed = 1\n",
    "#Hyperparameter\n",
    "lr = 3e-4\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 256\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "\n",
    "random_collect = 10000\n",
    "\n",
    "# RED-Q Parameter\n",
    "N = 2\n",
    "M = 2\n",
    "G = 1\n",
    "\n",
    "#writer = SummaryWriter(\"runs/\"+args.info)\n",
    "env = gym.make(env_name)\n",
    "action_high = env.action_space.high[0]\n",
    "action_low = env.action_space.low[0]\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "agent = REDQ_Agent(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              random_seed=seed,\n",
    "              device=device,\n",
    "              action_prior=\"uniform\", N=N, M=M, G=G)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable OFENet Parameter: \", count_parameters(agent.ofenet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "every-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding random samples to buffer done! Buffer size:  10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012951629236340523\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "fill_buffer(samples=random_collect)\n",
    "start_time = time.time()\n",
    "agent = pretrain_ofenet(agent, epochs=random_collect)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "hybrid-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training took: 2.246246894200643\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-training took: {}\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-concentration",
   "metadata": {},
   "source": [
    "# paper achieves loss of ~ 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.ofenet.state_dict(), \"ofenet_params_hopper.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "public-focus",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OFENet:\n\tMissing key(s) in state_dict: \"state_layer_block.0.layer.bias\", \"state_layer_block.1.layer.bias\", \"state_layer_block.2.layer.bias\", \"state_layer_block.3.layer.bias\", \"state_layer_block.4.layer.bias\", \"state_layer_block.5.layer.bias\", \"state_layer_block.6.layer.bias\", \"state_layer_block.7.layer.bias\", \"action_layer_block.0.layer.bias\", \"action_layer_block.1.layer.bias\", \"action_layer_block.2.layer.bias\", \"action_layer_block.3.layer.bias\", \"action_layer_block.4.layer.bias\", \"action_layer_block.5.layer.bias\", \"action_layer_block.6.layer.bias\", \"action_layer_block.7.layer.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-9f1454b35264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ofenet_params_cheetah.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OFENet:\n\tMissing key(s) in state_dict: \"state_layer_block.0.layer.bias\", \"state_layer_block.1.layer.bias\", \"state_layer_block.2.layer.bias\", \"state_layer_block.3.layer.bias\", \"state_layer_block.4.layer.bias\", \"state_layer_block.5.layer.bias\", \"state_layer_block.6.layer.bias\", \"state_layer_block.7.layer.bias\", \"action_layer_block.0.layer.bias\", \"action_layer_block.1.layer.bias\", \"action_layer_block.2.layer.bias\", \"action_layer_block.3.layer.bias\", \"action_layer_block.4.layer.bias\", \"action_layer_block.5.layer.bias\", \"action_layer_block.6.layer.bias\", \"action_layer_block.7.layer.bias\". "
     ]
    }
   ],
   "source": [
    "agent.ofenet.load_state_dict(torch.load(\"ofenet_params_cheetah.pth\"))\n",
    "agent.ofenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "rocky-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Frame: [11000/1000000] Reward: -1153.70  Average100 Score: -1153.70 ofenet_loss: 0.015, a_loss: 11.435, c_loss: 10.227\n",
      "Episode 2 Frame: [12000/1000000] Reward: -1195.26  Average100 Score: -1174.48 ofenet_loss: 0.014, a_loss: 30.108, c_loss: 15.601\n",
      "Episode 3 Frame: [13000/1000000] Reward: -1235.17  Average100 Score: -1194.71 ofenet_loss: 0.018, a_loss: 51.045, c_loss: 20.564\n",
      "Episode 4 Frame: [14000/1000000] Reward: -1176.32  Average100 Score: -1190.11 ofenet_loss: 0.016, a_loss: 75.323, c_loss: 24.623\n",
      "Episode 5 Frame: [15000/1000000] Reward: -1204.55  Average100 Score: -1193.00 ofenet_loss: 0.015, a_loss: 98.705, c_loss: 24.785\n",
      "Episode 6 Frame: [16000/1000000] Reward: -1338.96  Average100 Score: -1217.33 ofenet_loss: 0.017, a_loss: 128.636, c_loss: 22.149\n",
      "Episode 7 Frame: [17000/1000000] Reward: -1211.28  Average100 Score: -1216.46 ofenet_loss: 0.016, a_loss: 151.447, c_loss: 34.513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-dfaac6d5b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_collect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training took {} min!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-94c5c379b4c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(steps, precollected, print_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mofenet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-2021dfac2632>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mofenet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ofenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mofenet_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-35c3d9bed107>\u001b[0m in \u001b[0;36mtrain_ofenet\u001b[0;34m(self, experiences, optim)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mofenet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mofenet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adam does not support sparse gradients, please consider SparseAdam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retains_grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n\u001b[1;32m    949\u001b[0m                           \u001b[0;34m\"attribute won't be populated during autograd.backward(). If you indeed want the gradient \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "scores = train(max_steps, random_collect)\n",
    "t1 = time.time()\n",
    "env.close()\n",
    "print(\"training took {} min!\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-ordinary",
   "metadata": {},
   "source": [
    "# Actor loss is the problem!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
